# Data Source

The data used for this project is sourced from a crowd-sourcing platform specifically designed for labeling medical images. The dataset comprises two main categories of data:

    Crowd Data: This category includes data provided by users of the platform who are presented with various medical images and asked to vote on whether they believe the image exhibits a medical abnormality or appears normal and healthy.

    Expert Data: The expert data consists of classifications provided by medical professionals who specialize in relevant fields. Similar to the crowd data, these experts were asked to provide binary 'yes' or 'no' labels for the images.

# Objective

The primary objective of this project is to conduct an extensive exploratory analysis of the crowd data, expert data, and their interactions. The focus of the analysis is to assess the level of agreement between the crowd and expert labelers, particularly in relation to the expert majority opinion.

The project aims to demonstrate that the crowd exhibits a higher level of agreement with the expert majority compared to the agreement observed among the experts themselves.

Through this analysis, we seek to shed light on the effectiveness of utilizing crowd-sourced data for medical image labeling, potentially indicating its potential as a valuable resource in medical diagnostics and decision-making processes.
